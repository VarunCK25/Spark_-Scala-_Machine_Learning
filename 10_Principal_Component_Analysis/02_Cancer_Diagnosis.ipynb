{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __<h3>DATASET DESCRIPTION__\n",
    " \n",
    " \n",
    " Here is a description of the dataset we will be using:\n",
    "\n",
    " Breast Cancer Wisconsin (Diagnostic) Database\n",
    "\n",
    " Notes\n",
    " -----\n",
    " Data Set Characteristics:\n",
    "     :Number of Instances: 569\n",
    "\n",
    "     :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "     :Attribute Information:\n",
    "         - radius (mean of distances from center to points on the perimeter)\n",
    "         - texture (standard deviation of gray-scale values)\n",
    "         - perimeter\n",
    "         - area\n",
    "         - smoothness (local variation in radius lengths)\n",
    "         - compactness (perimeter^2 / area - 1.0)\n",
    "         - concavity (severity of concave portions of the contour)\n",
    "         - concave points (number of concave portions of the contour)\n",
    "         - symmetry\n",
    "         - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "         The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "         largest values) of these features were computed for each image,\n",
    "         resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "         13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "         - class:\n",
    "                 - WDBC-Malignant\n",
    "                 - WDBC-Benign\n",
    "\n",
    "     :Summary Statistics:\n",
    "\n",
    "     ===================================== ======= ========\n",
    "                                            Min     Max\n",
    "     ===================================== ======= ========\n",
    "     radius (mean):                         6.981   28.11\n",
    "     texture (mean):                        9.71    39.28\n",
    "     perimeter (mean):                      43.79   188.5\n",
    "     area (mean):                           143.5   2501.0\n",
    "     smoothness (mean):                     0.053   0.163\n",
    "     compactness (mean):                    0.019   0.345\n",
    "     concavity (mean):                      0.0     0.427\n",
    "     concave points (mean):                 0.0     0.201\n",
    "     symmetry (mean):                       0.106   0.304\n",
    "     fractal dimension (mean):              0.05    0.097\n",
    "     radius (standard error):               0.112   2.873\n",
    "     texture (standard error):              0.36    4.885\n",
    "     perimeter (standard error):            0.757   21.98\n",
    "     area (standard error):                 6.802   542.2\n",
    "     smoothness (standard error):           0.002   0.031\n",
    "     compactness (standard error):          0.002   0.135\n",
    "     concavity (standard error):            0.0     0.396\n",
    "     concave points (standard error):       0.0     0.053\n",
    "     symmetry (standard error):             0.008   0.079\n",
    "     fractal dimension (standard error):    0.001   0.03\n",
    "     radius (worst):                        7.93    36.04\n",
    "     texture (worst):                       12.02   49.54\n",
    "     perimeter (worst):                     50.41   251.2\n",
    "     area (worst):                          185.2   4254.0\n",
    "     smoothness (worst):                    0.071   0.223\n",
    "     compactness (worst):                   0.027   1.058\n",
    "     concavity (worst):                     0.0     1.252\n",
    "     concave points (worst):                0.0     0.291\n",
    "     symmetry (worst):                      0.156   0.664\n",
    "     fractal dimension (worst):             0.055   0.208\n",
    "     ===================================== ======= ========\n",
    "\n",
    "     :Missing Attribute Values: None\n",
    "\n",
    "     :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "     :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "     :Donor: Nick Street\n",
    "\n",
    "     :Date: November, 1995\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577783867767)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 14:47:57 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@ab81fb\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"cancer\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read the cancer data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [mean radius: int, mean texture: double ... 28 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"Cancer_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 569\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: integer (nullable = true)\n",
      " |-- mean texture: double (nullable = true)\n",
      " |-- mean perimeter: double (nullable = true)\n",
      " |-- mean area: double (nullable = true)\n",
      " |-- mean smoothness: double (nullable = true)\n",
      " |-- mean compactness: double (nullable = true)\n",
      " |-- mean concavity: double (nullable = true)\n",
      " |-- mean concave points: double (nullable = true)\n",
      " |-- mean symmetry: double (nullable = true)\n",
      " |-- mean fractal dimension: double (nullable = true)\n",
      " |-- radius error: double (nullable = true)\n",
      " |-- texture error: double (nullable = true)\n",
      " |-- perimeter error: double (nullable = true)\n",
      " |-- area error: double (nullable = true)\n",
      " |-- smoothness error: double (nullable = true)\n",
      " |-- compactness error: double (nullable = true)\n",
      " |-- concavity error: double (nullable = true)\n",
      " |-- concave points error: double (nullable = true)\n",
      " |-- symmetry error: double (nullable = true)\n",
      " |-- fractal dimension error: double (nullable = true)\n",
      " |-- worst radius: double (nullable = true)\n",
      " |-- worst texture: double (nullable = true)\n",
      " |-- worst perimeter: double (nullable = true)\n",
      " |-- worst area: double (nullable = true)\n",
      " |-- worst smoothness: double (nullable = true)\n",
      " |-- worst compactness: double (nullable = true)\n",
      " |-- worst concavity: double (nullable = true)\n",
      " |-- worst concave points: double (nullable = true)\n",
      " |-- worst symmetry: double (nullable = true)\n",
      " |-- worst fractal dimension: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{PCA, StandardScaler, VectorAssembler}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{PCA,StandardScaler,VectorAssembler}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VectorAssembler to convert the input columns of the cancer data to a single output column of an array called \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4df0293e6b5d\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(data.columns).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [mean radius: int, mean texture: double ... 29 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: integer (nullable = true)\n",
      " |-- mean texture: double (nullable = true)\n",
      " |-- mean perimeter: double (nullable = true)\n",
      " |-- mean area: double (nullable = true)\n",
      " |-- mean smoothness: double (nullable = true)\n",
      " |-- mean compactness: double (nullable = true)\n",
      " |-- mean concavity: double (nullable = true)\n",
      " |-- mean concave points: double (nullable = true)\n",
      " |-- mean symmetry: double (nullable = true)\n",
      " |-- mean fractal dimension: double (nullable = true)\n",
      " |-- radius error: double (nullable = true)\n",
      " |-- texture error: double (nullable = true)\n",
      " |-- perimeter error: double (nullable = true)\n",
      " |-- area error: double (nullable = true)\n",
      " |-- smoothness error: double (nullable = true)\n",
      " |-- compactness error: double (nullable = true)\n",
      " |-- concavity error: double (nullable = true)\n",
      " |-- concave points error: double (nullable = true)\n",
      " |-- symmetry error: double (nullable = true)\n",
      " |-- fractal dimension error: double (nullable = true)\n",
      " |-- worst radius: double (nullable = true)\n",
      " |-- worst texture: double (nullable = true)\n",
      " |-- worst perimeter: double (nullable = true)\n",
      " |-- worst area: double (nullable = true)\n",
      " |-- worst smoothness: double (nullable = true)\n",
      " |-- worst compactness: double (nullable = true)\n",
      " |-- worst concavity: double (nullable = true)\n",
      " |-- worst concave points: double (nullable = true)\n",
      " |-- worst symmetry: double (nullable = true)\n",
      " |-- worst fractal dimension: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often its a good idea to normalize each feature to have unit standard deviation and/or zero mean, when using PCA. This is essentially a pre-step to PCA, but its not always necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using standard scaler to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_d70bad30cee5\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute summary statistics by fitting the StandardScaler. Basically create a new object called scalerModel by using scaler.fit() on the output of the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler_model: org.apache.spark.ml.feature.StandardScalerModel = stdScal_d70bad30cee5\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler_model = scaler.fit(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize each feature to have unit standard deviation. Use transform() off of this scalerModel object to create your scaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaledData: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaledData = scaler_model.transform(output).select(\"features\",\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[0.0,17.99,10.38,...|[0.0,5.1049235941...|\n",
      "|[1.0,20.57,17.77,...|[0.00608270930682...|\n",
      "|[2.0,19.69,21.25,...|[0.01216541861364...|\n",
      "|[3.0,11.42,20.38,...|[0.01824812792047...|\n",
      "|[4.0,20.29,14.34,...|[0.02433083722729...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new PCA() object that will take in the scaledFeatures and output the pcs features, use 4 principal components, Then fit this to the scaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 15:03:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-31 15:03:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-31 15:03:11 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-31 15:03:11 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pca: org.apache.spark.ml.feature.PCAModel = pca_8f535bf16297\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pca = new PCA().setInputCol(\"scaledFeatures\").setOutputCol(\"pcaFeatures\").setK(4).fit(scaledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and transforming the scaledData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcaDF: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pcaDF = pca.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|         pcaFeatures|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0.0,17.99,10.38,...|[0.0,5.1049235941...|[21.6219973823647...|\n",
      "|[1.0,20.57,17.77,...|[0.00608270930682...|[15.1217370347581...|\n",
      "|[2.0,19.69,21.25,...|[0.01216541861364...|[18.4325856097776...|\n",
      "|[3.0,11.42,20.38,...|[0.01824812792047...|[18.9549565028936...|\n",
      "|[4.0,20.29,14.34,...|[0.02433083722729...|[16.7333072691961...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the new pcaFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [pcaFeatures: vector]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = pcaDF.select(\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|pcaFeatures                                                                   |\n",
      "+------------------------------------------------------------------------------+\n",
      "|[21.62199738236476,8.516595739466684,-3.7318474175794782,-0.4181244970133412] |\n",
      "|[15.121737034758134,2.697138979042207,-2.3546461829874357,-2.59498897333438]  |\n",
      "|[18.432585609777654,5.697069543518227,-2.9058070696230303,-3.0552108608152326]|\n",
      "|[18.95495650289368,16.025442209800573,-5.934803967957989,-4.158068180951641]  |\n",
      "|[16.73330726919616,4.995746645493643,-0.998499731787013,-0.8269447324688084]  |\n",
      "+------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use .head() to confirm that our output column Array of pcaFeatures only has 4 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.sql.Row] = Array([[21.62199738236476,8.516595739466684,-3.7318474175794782,-0.4181244970133412]])\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
