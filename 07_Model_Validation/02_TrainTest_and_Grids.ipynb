{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577635298671)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 21:31:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6951b939\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements to setup ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder,TrainValidationSplit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"USA_Housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Avg Area Income: double (nullable = true)\n",
      " |-- Avg Area House Age: double (nullable = true)\n",
      " |-- Avg Area Number of Rooms: double (nullable = true)\n",
      " |-- Avg Area Number of Bedrooms: double (nullable = true)\n",
      " |-- Area Population: double (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See an example of what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(Avg Area Income, Avg Area House Age, Avg Area Number of Rooms, Avg Area Number of Bedrooms, Area Population, Price)\r\n",
       "firstRow: org.apache.spark.sql.Row = [79545.45857431678,5.682861321615587,7.009188142792237,4.09,23086.800502686456,1059033.5578701235]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colnames = data.columns\n",
    "val firstRow = data.head(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: Avg Area Income\n",
      "Column Data: 79545.45857431678\n",
      "\n",
      "Column Name: Avg Area House Age\n",
      "Column Data: 5.682861321615587\n",
      "\n",
      "Column Name: Avg Area Number of Rooms\n",
      "Column Data: 7.009188142792237\n",
      "\n",
      "Column Name: Avg Area Number of Bedrooms\n",
      "Column Data: 4.09\n",
      "\n",
      "Column Name: Area Population\n",
      "Column Data: 23086.800502686456\n",
      "\n",
      "Column Name: Price\n",
      "Column Data: 1059033.5578701235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i <- Range(0,colnames.size)){\n",
    "    println(s\"Column Name: ${colnames(i)}\")\n",
    "    println(s\"Column Data: ${firstRow(i)}\")\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up DataFrame for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = data.select(data(\"Price\").as(\"label\"),$\"Avg Area Income\",$\"Avg Area House Age\",$\"Avg Area Number of Rooms\"\n",
    "                     ,$\"Avg Area Number of Bedrooms\",$\"Area Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "An assembler converts the input values to a vector\n",
    "A vector is what the ML algorithm reads to train a model\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the input columns from which we are supposed to read the values, Set the name of the column where the vector will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(label, Avg Area Income, Avg Area House Age, Avg Area Number of Rooms, Avg Area Number of Bedrooms, Area Population)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ba4d25ed0c99\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\", \"Avg Area House Age\", \"Avg Area Number of Rooms\"\n",
    "                                                   , \"Avg Area Number of Bedrooms\", \"Area Population\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(df).select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array of the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,testing) = output.randomSplit(Array(0.7,0.3), seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|              5000|\n",
      "|   mean|  1232072.65414236|\n",
      "| stddev|353117.62658106035|\n",
      "|    min|15938.657923287848|\n",
      "|    max|2469065.5941747027|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|              3481|\n",
      "|   mean|1237811.1892187502|\n",
      "| stddev| 354963.1535248248|\n",
      "|    min|15938.657923287848|\n",
      "|    max|2370231.3201015536|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|              1519|\n",
      "|   mean|1218922.0020021829|\n",
      "| stddev|348611.31160445284|\n",
      "|    min| 151527.0826265551|\n",
      "|    max|2469065.5941747027|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_d41f9260b8b4\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETER GRID BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tlinReg_d41f9260b8b4-regParam: 1000.0\r\n",
       "}, {\r\n",
       "\tlinReg_d41f9260b8b4-regParam: 0.001\r\n",
       "})\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam,Array(1000,0.001)).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the estimator is simply the `linear regression`.\n",
    "<br>A TrainValidationSplit requires an `Estimator`, a set of `Estimator ParamMaps`, and an `Evaluator`.\n",
    "<br>`80%` of the data will be used for training and the remaining `20%` for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_c73b0dfec311\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainValidationSplit = (new TrainValidationSplit().setEstimator(lr)\n",
    "                                                      .setEvaluator(new RegressionEvaluator().setMetricName(\"r2\"))\n",
    "                                                      .setEstimatorParamMaps(paramGrid)\n",
    "                                                      .setTrainRatio(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can then treat this object as the new model and use fit on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 21:54:03 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-29 21:54:03 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-29 21:54:03 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-29 21:54:03 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_c73b0dfec311\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION USING THE TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test data. model is the model with combination of parameters that performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[48735.9245124086...| 151527.0826265551|  375881.466199561|\n",
      "|[40366.6162912572...|152071.87474956046|221510.06829972798|\n",
      "|[50926.7766338627...|211017.97049475575| 473496.9527376839|\n",
      "|[62173.5800990082...| 231189.8209898588| 301314.0310567827|\n",
      "|[55048.3320238013...| 288708.9121479006|401393.64051124407|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(testing).select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[Double] = Array(0.9278218053023559, 0.9277540425917379)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
