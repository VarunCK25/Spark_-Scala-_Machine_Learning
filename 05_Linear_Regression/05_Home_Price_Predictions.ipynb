{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the price of an house using Linear Regression Model and using One Hot Encoder for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-27 12:41:10 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7ef61506\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"homeprice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements to setup ML for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, OneHotEncoder}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.feature.{StringIndexer,VectorAssembler,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read in the homeprices csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [town: string, area: int ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"homeprices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the count of the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 13\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the count of the Dataframe by dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 13\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[monroe township,2600,550000]\n",
      "[monroe township,3000,565000]\n",
      "[monroe township,3200,610000]\n",
      "[monroe township,3600,680000]\n",
      "[monroe township,4000,725000]\n"
     ]
    }
   ],
   "source": [
    "for (row <- data.head(5)){\n",
    "    println(row)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|           town|area| price|\n",
      "+---------------+----+------+\n",
      "|monroe township|2600|550000|\n",
      "|monroe township|3000|565000|\n",
      "|monroe township|3200|610000|\n",
      "|monroe township|3600|680000|\n",
      "|monroe township|4000|725000|\n",
      "|   west windsor|2600|585000|\n",
      "|   west windsor|2800|615000|\n",
      "|   west windsor|3300|650000|\n",
      "|   west windsor|3600|710000|\n",
      "|    robinsville|2600|575000|\n",
      "|    robinsville|2900|600000|\n",
      "|    robinsville|3100|620000|\n",
      "|    robinsville|3600|695000|\n",
      "+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- town: string (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           town|count|\n",
      "+---------------+-----+\n",
      "|monroe township|    5|\n",
      "|   west windsor|    4|\n",
      "|    robinsville|    4|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"town\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the categotical column `town` from String type to Vector form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using String Indexer to convert categorical string columns to numerical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_64cc5c9186bb\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer().setInputCol(\"town\").setOutputCol(\"townInd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer_model: org.apache.spark.ml.feature.StringIndexerModel = strIdx_64cc5c9186bb\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer_model = indexer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer_df: org.apache.spark.sql.DataFrame = [town: string, area: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer_df = indexer_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+-------+\n",
      "|           town|area| price|townInd|\n",
      "+---------------+----+------+-------+\n",
      "|monroe township|2600|550000|    0.0|\n",
      "|monroe township|3000|565000|    0.0|\n",
      "|monroe township|3200|610000|    0.0|\n",
      "|monroe township|3600|680000|    0.0|\n",
      "|monroe township|4000|725000|    0.0|\n",
      "|   west windsor|2600|585000|    1.0|\n",
      "|   west windsor|2800|615000|    1.0|\n",
      "|   west windsor|3300|650000|    1.0|\n",
      "|   west windsor|3600|710000|    1.0|\n",
      "|    robinsville|2600|575000|    2.0|\n",
      "|    robinsville|2900|600000|    2.0|\n",
      "|    robinsville|3100|620000|    2.0|\n",
      "|    robinsville|3600|695000|    2.0|\n",
      "+---------------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using One Hot Encoder to convert categorical numeric type columns to Vector type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_112ccab3b4db\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoder().setInputCol(\"townInd\").setOutputCol(\"townVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_df: org.apache.spark.sql.DataFrame = [town: string, area: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder_df = encoder.transform(indexer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+-------+-------------+\n",
      "|           town|area| price|townInd|      townVec|\n",
      "+---------------+----+------+-------+-------------+\n",
      "|monroe township|2600|550000|    0.0|(2,[0],[1.0])|\n",
      "|monroe township|3000|565000|    0.0|(2,[0],[1.0])|\n",
      "|monroe township|3200|610000|    0.0|(2,[0],[1.0])|\n",
      "|monroe township|3600|680000|    0.0|(2,[0],[1.0])|\n",
      "|monroe township|4000|725000|    0.0|(2,[0],[1.0])|\n",
      "|   west windsor|2600|585000|    1.0|(2,[1],[1.0])|\n",
      "|   west windsor|2800|615000|    1.0|(2,[1],[1.0])|\n",
      "|   west windsor|3300|650000|    1.0|(2,[1],[1.0])|\n",
      "|   west windsor|3600|710000|    1.0|(2,[1],[1.0])|\n",
      "|    robinsville|2600|575000|    2.0|    (2,[],[])|\n",
      "|    robinsville|2900|600000|    2.0|    (2,[],[])|\n",
      "|    robinsville|3100|620000|    2.0|    (2,[],[])|\n",
      "|    robinsville|3600|695000|    2.0|    (2,[],[])|\n",
      "+---------------+----+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling all the features to a single vector column \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2ba78357b57a\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"townVec\",\"area\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [town: string, area: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(encoder_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+-------+-------------+----------------+\n",
      "|           town|area| price|townInd|      townVec|        features|\n",
      "+---------------+----+------+-------+-------------+----------------+\n",
      "|monroe township|2600|550000|    0.0|(2,[0],[1.0])|[1.0,0.0,2600.0]|\n",
      "|monroe township|3000|565000|    0.0|(2,[0],[1.0])|[1.0,0.0,3000.0]|\n",
      "|monroe township|3200|610000|    0.0|(2,[0],[1.0])|[1.0,0.0,3200.0]|\n",
      "|monroe township|3600|680000|    0.0|(2,[0],[1.0])|[1.0,0.0,3600.0]|\n",
      "|monroe township|4000|725000|    0.0|(2,[0],[1.0])|[1.0,0.0,4000.0]|\n",
      "|   west windsor|2600|585000|    1.0|(2,[1],[1.0])|[0.0,1.0,2600.0]|\n",
      "|   west windsor|2800|615000|    1.0|(2,[1],[1.0])|[0.0,1.0,2800.0]|\n",
      "|   west windsor|3300|650000|    1.0|(2,[1],[1.0])|[0.0,1.0,3300.0]|\n",
      "|   west windsor|3600|710000|    1.0|(2,[1],[1.0])|[0.0,1.0,3600.0]|\n",
      "|    robinsville|2600|575000|    2.0|    (2,[],[])|[0.0,0.0,2600.0]|\n",
      "|    robinsville|2900|600000|    2.0|    (2,[],[])|[0.0,0.0,2900.0]|\n",
      "|    robinsville|3100|620000|    2.0|    (2,[],[])|[0.0,0.0,3100.0]|\n",
      "|    robinsville|3600|695000|    2.0|    (2,[],[])|[0.0,0.0,3600.0]|\n",
      "+---------------+----+------+-------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_data: org.apache.spark.sql.DataFrame = [price: int, features: vector]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_data = output.select(\"price\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "| price|        features|\n",
      "+------+----------------+\n",
      "|550000|[1.0,0.0,2600.0]|\n",
      "|565000|[1.0,0.0,3000.0]|\n",
      "|610000|[1.0,0.0,3200.0]|\n",
      "|680000|[1.0,0.0,3600.0]|\n",
      "|725000|[1.0,0.0,4000.0]|\n",
      "|585000|[0.0,1.0,2600.0]|\n",
      "|615000|[0.0,1.0,2800.0]|\n",
      "|650000|[0.0,1.0,3300.0]|\n",
      "|710000|[0.0,1.0,3600.0]|\n",
      "|575000|[0.0,0.0,2600.0]|\n",
      "|600000|[0.0,0.0,2900.0]|\n",
      "|620000|[0.0,0.0,3100.0]|\n",
      "|695000|[0.0,0.0,3600.0]|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the resultant data into training data and testing data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<b>Training data is to train the model</b>\n",
    "<b>Testing data is to test the builted model</b>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [price: int, features: vector]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [price: int, features: vector]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data,test_data) = final_data.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             price|\n",
      "+-------+------------------+\n",
      "|  count|                13|\n",
      "|   mean| 629230.7692307692|\n",
      "| stddev|57621.109913748696|\n",
      "|    min|            550000|\n",
      "|    max|            725000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            price|\n",
      "+-------+-----------------+\n",
      "|  count|                8|\n",
      "|   mean|         638750.0|\n",
      "| stddev|52218.63378417436|\n",
      "|    min|           575000|\n",
      "|    max|           710000|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            price|\n",
      "+-------+-----------------+\n",
      "|  count|                5|\n",
      "|   mean|         614000.0|\n",
      "| stddev|68684.05928597989|\n",
      "|    min|           550000|\n",
      "|    max|           725000|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_18a6c5262406\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LinearRegression().setLabelCol(\"price\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model and fitting the training data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-27 12:44:24 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-27 12:44:24 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-27 12:44:24 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-27 12:44:24 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_18a6c5262406\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the training summary of the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@37ce4964\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| 2978.3393501781393|\n",
      "| 1245.4873646186898|\n",
      "| -7545.126353791449|\n",
      "|  7563.176895305514|\n",
      "|-16642.599277977366|\n",
      "|                0.0|\n",
      "|  4566.787003612495|\n",
      "| 7833.9350180530455|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6046.931407942087\n",
      "Mean Squared Error: 6.046931407942254E7\n",
      "Root Mean Squared Error: 7776.201777180331\n",
      "R Squared Error: 0.9746559521867515\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${trainingSummary.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_results: org.apache.spark.ml.regression.LinearRegressionSummary = org.apache.spark.ml.regression.LinearRegressionSummary@44153afe\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the co-effecients and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-10433.212996387569,11732.851985559524,118.41155234656571]\n",
      "Intercept: 264151.62454875093\n"
     ]
    }
   ],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients}\")\n",
    "println(s\"Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -11588.44765343424|\n",
      "| -43953.06859206059|\n",
      "| -22635.37906137365|\n",
      "|-11227.436823104625|\n",
      "|-2364.6209386262344|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model by checking the different types of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 18353.790613719866\n",
      "Mean Squared Error: 5.420343025454074E8\n",
      "Root Mean Squared Error: 23281.630152233916\n",
      "R Squared Error: 0.8563767083875444\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${test_results.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${test_results.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${test_results.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${test_results.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the predictions from the builted model without label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlabelled_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unlabelled_data = test_data.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.transform(unlabelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|        features|       prediction|\n",
      "+----------------+-----------------+\n",
      "|[1.0,0.0,2600.0]|561588.4476534342|\n",
      "|[1.0,0.0,3000.0]|608953.0685920606|\n",
      "|[1.0,0.0,3200.0]|632635.3790613736|\n",
      "|[0.0,0.0,3100.0]|631227.4368231046|\n",
      "|[1.0,0.0,4000.0]|727364.6209386262|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the created spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
