{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine a dataset with Ecommerce Customer Data for a company's website and mobile app. Then we want to see if we can build a regression model that will predict the customer's yearly spend on the company's product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 12:10:38 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@148e681e\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"ECommerce\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements to setup ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read in the Ecommerce Customers csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Email: string, Address: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"Ecommerce_Customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the count of the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 500\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mstephenson@fernandez.com,835 Frank TunnelWrightmouth, MI 82180-9605,Violet,34.49726772511229,12.65565114916675,39.57766801952616,4.0826206329529615,587.9510539684005]\n",
      "[hduke@hotmail.com,4547 Archer CommonDiazchester, CA 06566-8576,DarkGreen,31.92627202636016,11.109460728682564,37.268958868297744,2.66403418213262,392.2049334443264]\n",
      "[pallen@yahoo.com,24645 Valerie Unions Suite 582Cobbborough, DC 99414-7564,Bisque,33.000914755642675,11.330278057777512,37.110597442120856,4.104543202376424,487.54750486747207]\n",
      "[riverarebecca@gmail.com,1414 David ThroughwayPort Jason, OH 22070-1220,SaddleBrown,34.30555662975554,13.717513665142507,36.72128267790313,3.120178782748092,581.8523440352177]\n",
      "[mstephens@davidson-herman.com,14023 Rodriguez PassagePort Jacobville, PR 37242-1057,MediumAquaMarine,33.33067252364639,12.795188551078114,37.53665330059473,4.446308318351434,599.4060920457634]\n"
     ]
    }
   ],
   "source": [
    "for (row <- data.head(5)){\n",
    "    println(row)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "|               Email|             Address|   Avatar|Avg Session Length|       Time on App|   Time on Website|Length of Membership|Yearly Amount Spent|\n",
      "+--------------------+--------------------+---------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "|mstephenson@ferna...|835 Frank TunnelW...|   Violet| 34.49726772511229| 12.65565114916675| 39.57766801952616|  4.0826206329529615|  587.9510539684005|\n",
      "|   hduke@hotmail.com|4547 Archer Commo...|DarkGreen| 31.92627202636016|11.109460728682564|37.268958868297744|    2.66403418213262|  392.2049334443264|\n",
      "|    pallen@yahoo.com|24645 Valerie Uni...|   Bisque|33.000914755642675|11.330278057777512|37.110597442120856|   4.104543202376424| 487.54750486747207|\n",
      "+--------------------+--------------------+---------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See an example of what the data looks like by printing out a Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Email\n",
      "Data: mstephenson@fernandez.com\n",
      "\n",
      "Column: Address\n",
      "Data: 835 Frank TunnelWrightmouth, MI 82180-9605\n",
      "\n",
      "Column: Avatar\n",
      "Data: Violet\n",
      "\n",
      "Column: Avg Session Length\n",
      "Data: 34.49726772511229\n",
      "\n",
      "Column: Time on App\n",
      "Data: 12.65565114916675\n",
      "\n",
      "Column: Time on Website\n",
      "Data: 39.57766801952616\n",
      "\n",
      "Column: Length of Membership\n",
      "Data: 4.0826206329529615\n",
      "\n",
      "Column: Yearly Amount Spent\n",
      "Data: 587.9510539684005\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(Email, Address, Avatar, Avg Session Length, Time on App, Time on Website, Length of Membership, Yearly Amount Spent)\r\n",
       "firstRow: org.apache.spark.sql.Row = [mstephenson@fernandez.com,835 Frank TunnelWrightmouth, MI 82180-9605,Violet,34.49726772511229,12.65565114916675,39.57766801952616,4.0826206329529615,587.9510539684005]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colnames = data.columns\n",
    "val firstRow = data.head(1)(0)\n",
    "\n",
    "for (i <- Range(0,colnames.size)){\n",
    "    println(\"Column: \"+colnames(i))\n",
    "    println(\"Data: \"+firstRow(i))\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the string columns and converting the dataframe to ML acceptable format ---> i.e., (\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered_data: org.apache.spark.sql.DataFrame = [Avg Session Length: double, Time on App: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered_data = data.select(\"Avg Session Length\", \"Time on App\", \"Time on Website\", \"Length of Membership\", \"Yearly Amount Spent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ec652b3bcb54\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Session Length\", \"Time on App\", \"Time on Website\"\n",
    "                                                         , \"Length of Membership\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [Avg Session Length: double, Time on App: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+--------------------+-------------------+--------------------+\n",
      "|Avg Session Length|       Time on App|   Time on Website|Length of Membership|Yearly Amount Spent|            features|\n",
      "+------------------+------------------+------------------+--------------------+-------------------+--------------------+\n",
      "| 34.49726772511229| 12.65565114916675| 39.57766801952616|  4.0826206329529615|  587.9510539684005|[34.4972677251122...|\n",
      "| 31.92627202636016|11.109460728682564|37.268958868297744|    2.66403418213262|  392.2049334443264|[31.9262720263601...|\n",
      "|33.000914755642675|11.330278057777512|37.110597442120856|   4.104543202376424| 487.54750486747207|[33.0009147556426...|\n",
      "+------------------+------------------+------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_data: org.apache.spark.sql.DataFrame = [Yearly Amount Spent: double, features: vector]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_data = output.select(\"Yearly Amount Spent\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------------------------------------------------------------+\n",
      "|Yearly Amount Spent|features                                                                    |\n",
      "+-------------------+----------------------------------------------------------------------------+\n",
      "|587.9510539684005  |[34.49726772511229,12.65565114916675,39.57766801952616,4.0826206329529615]  |\n",
      "|392.2049334443264  |[31.92627202636016,11.109460728682564,37.268958868297744,2.66403418213262]  |\n",
      "|487.54750486747207 |[33.000914755642675,11.330278057777512,37.110597442120856,4.104543202376424]|\n",
      "|581.8523440352177  |[34.30555662975554,13.717513665142507,36.72128267790313,3.120178782748092]  |\n",
      "|599.4060920457634  |[33.33067252364639,12.795188551078114,37.53665330059473,4.446308318351434]  |\n",
      "+-------------------+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the resultane data into training data and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<b>Training data is to train the model</b>\n",
    "<b>Testing data is to test the builted model</b>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the total data to 70% and 30% for training data and testing data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Yearly Amount Spent: double, features: vector]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Yearly Amount Spent: double, features: vector]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data,test_data) = final_data.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                500|\n",
      "|   mean|  499.3140382585909|\n",
      "| stddev|   79.3147815497068|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                349|\n",
      "|   mean| 498.91895574689846|\n",
      "| stddev|  80.16344725795035|\n",
      "|    min|   266.086340948469|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                151|\n",
      "|   mean|  500.2271759842895|\n",
      "| stddev|  77.57302061278227|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  725.5848140556806|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_ad567351be48\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LinearRegression().setLabelCol(\"Yearly Amount Spent\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model and fitting the training data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 12:28:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-26 12:28:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-26 12:28:55 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-26 12:28:55 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_ad567351be48\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the training summary of the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@4414fb8e\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-17.132000238974456|\n",
      "| -5.249605680316165|\n",
      "| -5.707320807003555|\n",
      "| 16.638244206776164|\n",
      "| -9.089548631163439|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummary.residuals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 8.020052223176389\n",
      "Mean Squared Error: 102.25435605992648\n",
      "Root Mean Squared Error: 10.11208959908517\n",
      "R Squared Error: 0.984042118657839\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${trainingSummary.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_results: org.apache.spark.ml.regression.LinearRegressionSummary = org.apache.spark.ml.regression.LinearRegressionSummary@2ba3a0c4\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the co-effecients and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [25.37725532838959,37.99550195624851,0.47167795343915264,61.91345660542813]\n",
      "Intercept: -1033.6792488814692\n"
     ]
    }
   ],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients}\")\n",
    "println(s\"Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| 1.4054810152236428|\n",
      "|-6.4842896796715195|\n",
      "| -8.494262494387215|\n",
      "|0.16506940239332835|\n",
      "| -6.569702155294692|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model by checking the different types of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.628800374545236\n",
      "Mean Squared Error: 92.02837009666581\n",
      "Root Mean Squared Error: 9.593141826151943\n",
      "R Squared Error: 0.9846047759701744\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${test_results.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${test_results.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${test_results.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${test_results.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the predictions from the builted model without label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlabelled_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unlabelled_data = test_data.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.transform(unlabelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+------------------+\n",
      "|features                                                                    |prediction        |\n",
      "+----------------------------------------------------------------------------+------------------+\n",
      "|[32.83694076702139,10.25654903128796,36.143908456341634,0.7895199078816915] |255.2651012748322 |\n",
      "|[32.529768731474434,11.747731701242175,36.93988205032054,0.8015157200042076]|305.2462975414792 |\n",
      "|[34.083663301629485,8.668349517101323,35.90675636579306,2.2524459633808416] |317.0220090524206 |\n",
      "|[32.40237101796123,10.875559548189257,37.78114255999947,1.9140899242311]    |338.1547932391288 |\n",
      "|[33.50370517913956,12.399436075147092,35.01280603355904,0.9686221157417688] |364.1611415703808 |\n",
      "|[33.07773079450243,11.466984219092824,35.675727630820134,1.8092295917763102]|370.2797045192233 |\n",
      "|[32.58249357081853,11.739743796165989,36.85481082475086,2.1820169698233887] |391.7122827915598 |\n",
      "|[34.18777482695728,10.320116255059194,37.45340510152358,2.0948917061051766] |373.398571576158  |\n",
      "|[32.72852076313786,10.131712461927217,34.84561239331277,3.2877018240744085] |401.82917106891296|\n",
      "|[33.73264839262621,12.138793877121687,36.853882459920776,1.62341960945622]  |401.47703218312245|\n",
      "|[32.07759004432913,10.347876945661575,39.045155696388115,3.4345597225409197]|404.59692619565635|\n",
      "|[33.34450868519732,10.969802874106238,35.9745781075859,2.627624971434411]   |408.96978840711836|\n",
      "|[33.68093694960616,11.201569884684467,37.83544772741218,2.208813678005546]  |401.261194608853  |\n",
      "|[33.59048580327403,10.942069621017652,36.170494202480995,2.783963128204522] |423.93011865315657|\n",
      "|[34.33667722314705,11.246812606011584,38.68258378408295,2.0947616690074846] |412.95932415793027|\n",
      "|[33.58737339024554,9.95399500605983,37.34573893439929,3.2156668270219866]   |413.5913484405753 |\n",
      "|[32.29524762622046,11.031358340409188,38.252978273334655,3.1074686827074625]|415.4647073702449 |\n",
      "|[33.23560650069385,11.223368888365105,37.692300861282355,2.59418972057778]  |414.580637856001  |\n",
      "|[31.661049822746076,11.398064190096813,36.59445670006827,3.1983992716849063]|418.1512196252936 |\n",
      "|[31.260646869879523,13.266760352944493,36.971195097457155,2.267251114447051]|421.51923853716335|\n",
      "+----------------------------------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the created spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
