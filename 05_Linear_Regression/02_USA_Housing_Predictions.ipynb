{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA Housing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@70e6b3f6\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize logger to ERROR to see less warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read in the USA Housing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"USA_Housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Avg Area Income: double (nullable = true)\n",
      " |-- Avg Area House Age: double (nullable = true)\n",
      " |-- Avg Area Number of Rooms: double (nullable = true)\n",
      " |-- Avg Area Number of Bedrooms: double (nullable = true)\n",
      " |-- Area Population: double (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------------+---------------------------+------------------+------------------+\n",
      "|   Avg Area Income|Avg Area House Age|Avg Area Number of Rooms|Avg Area Number of Bedrooms|   Area Population|             Price|\n",
      "+------------------+------------------+------------------------+---------------------------+------------------+------------------+\n",
      "| 79545.45857431678| 5.682861321615587|       7.009188142792237|                       4.09|23086.800502686456|1059033.5578701235|\n",
      "| 79248.64245482568|6.0028998082752425|       6.730821019094919|                       3.09| 40173.07217364482|  1505890.91484695|\n",
      "|61287.067178656784| 5.865889840310001|       8.512727430375099|                       5.13| 36882.15939970458|1058987.9878760849|\n",
      "+------------------+------------------+------------------------+---------------------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See an example of what the data looks like by printing out a Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(Avg Area Income, Avg Area House Age, Avg Area Number of Rooms, Avg Area Number of Bedrooms, Area Population, Price)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colnames = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstRow: org.apache.spark.sql.Row = [79545.45857431678,5.682861321615587,7.009188142792237,4.09,23086.800502686456,1059033.5578701235]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstRow = data.head(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Data Row\n",
      "Avg Area House Age\n",
      "5.682861321615587\n",
      "\n",
      "\n",
      "Avg Area Number of Rooms\n",
      "7.009188142792237\n",
      "\n",
      "\n",
      "Avg Area Number of Bedrooms\n",
      "4.09\n",
      "\n",
      "\n",
      "Area Population\n",
      "23086.800502686456\n",
      "\n",
      "\n",
      "Price\n",
      "1059033.5578701235\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstRow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up DataFrame for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things we need to do before Spark can accept the data!\n",
    "```\n",
    " It needs to be in the form of two columns\n",
    " **(\"label\",\"features\")**\n",
    "```\n",
    "\n",
    "This will allow us to join multiple feature columns into a single column of an array of feautre values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Linear Regression ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\r\n",
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder,TrainValidationSplit}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Price to label column for naming convention. Grab only numerical columns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = data.select(data(\"Price\").as(\"label\"), $\"Avg Area Income\", $\"Avg Area House Age\", $\"Avg Area Number of Bedrooms\",\n",
    "                     $\"Avg Area Number of Rooms\",$\"Area Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+---------------------------+------------------------+------------------+\n",
      "|             label|   Avg Area Income|Avg Area House Age|Avg Area Number of Bedrooms|Avg Area Number of Rooms|   Area Population|\n",
      "+------------------+------------------+------------------+---------------------------+------------------------+------------------+\n",
      "|1059033.5578701235| 79545.45857431678| 5.682861321615587|                       4.09|       7.009188142792237|23086.800502686456|\n",
      "|  1505890.91484695| 79248.64245482568|6.0028998082752425|                       3.09|       6.730821019094919| 40173.07217364482|\n",
      "|1058987.9878760849|61287.067178656784| 5.865889840310001|                       5.13|       8.512727430375099| 36882.15939970458|\n",
      "+------------------+------------------+------------------+---------------------------+------------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **An assembler converts the input values to a vector**\n",
    "- **A vector is what the ML algorithm reads to train a model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br>Set the input columns from which we are supposed to read the values </br>\n",
    "### Set the name of the column where the vector will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0aa8a599ed80\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Bedrooms\"\n",
    "                                                        ,\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the assembler to transform our DataFrame to the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------------------------------------------+\n",
      "|label             |features                                                                       |\n",
      "+------------------+-------------------------------------------------------------------------------+\n",
      "|1059033.5578701235|[79545.45857431678,5.682861321615587,4.09,7.009188142792237,23086.800502686456]|\n",
      "|1505890.91484695  |[79248.64245482568,6.0028998082752425,3.09,6.730821019094919,40173.07217364482]|\n",
      "|1058987.9878760849|[61287.067178656784,5.865889840310001,5.13,8.512727430375099,36882.15939970458]|\n",
      "|1260616.8066294468|[63345.24004622798,7.1882360945186425,3.26,5.586728664827653,34310.24283090706]|\n",
      "|630943.4893385402 |[59982.19722570803,5.040554523106283,4.23,7.839387785120487,26354.109472103148]|\n",
      "+------------------+-------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the resultane data into training data and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<b>Training data is to train the model</b>\n",
    "<b>Testing data is to test the builted model</b>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the total data to 70% and 30% for training data and testing data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data, test_data) = output.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|              3518|\n",
      "|   mean|1230560.7797947223|\n",
      "| stddev|352169.89182459255|\n",
      "|    min|15938.657923287848|\n",
      "|    max|2370231.3201015536|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|              1482|\n",
      "|   mean|1235661.5704412628|\n",
      "| stddev|355450.80578399764|\n",
      "|    min|211017.97049475575|\n",
      "|    max|2469065.5941747027|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_fd5615b54f72\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LinearRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear regression model and fitting the training data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 11:16:54 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-26 11:16:54 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-26 11:16:54 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-26 11:16:54 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_fd5615b54f72\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the training summary of the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@15dc3f01\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         residuals|\n",
      "+------------------+\n",
      "|  -53139.342100527|\n",
      "|-78122.98662759717|\n",
      "|-68321.06200949212|\n",
      "| 51200.89641396806|\n",
      "|-220668.0145468796|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummary.residuals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 81244.25778543952\n",
      "Mean Squared Error: 1.0179945237000706E10\n",
      "Root Mean Squared Error: 100895.7146612318\n",
      "R Squared Error: 0.9178959726345578\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${trainingSummary.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_results: org.apache.spark.ml.regression.LinearRegressionSummary = org.apache.spark.ml.regression.LinearRegressionSummary@153ce85a\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the co-effecients and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeffecients: [21.45412372772751,163929.87098514492,762.0527910566846,122231.78361454248,15.272557127819224]\n",
      "Intercept: -2629250.4438277474\n"
     ]
    }
   ],
   "source": [
    "println(s\"Coeffecients: ${lrModel.coefficients}\")\n",
    "println(s\"Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-260806.03998372125|\n",
      "| -63123.19783675301|\n",
      "|  1119.126134990016|\n",
      "| -66822.19692723238|\n",
      "| 31521.908127737057|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model by checking the different types of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 81740.34615371792\n",
      "Mean Squared Error: 1.0340750449878592E10\n",
      "Root Mean Squared Error: 101689.4805271351\n",
      "R Squared Error: 0.9180995672498196\n"
     ]
    }
   ],
   "source": [
    "println(s\"Mean Absolute Error: ${test_results.meanAbsoluteError}\")\n",
    "println(s\"Mean Squared Error: ${test_results.meanSquaredError}\")\n",
    "println(s\"Root Mean Squared Error: ${test_results.rootMeanSquaredError}\")\n",
    "println(s\"R Squared Error: ${test_results.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the predictions from the builted model without label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlabelled_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unlabelled_data = test_data.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.transform(unlabelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+------------------+\n",
      "|features                                                                         |prediction        |\n",
      "+---------------------------------------------------------------------------------+------------------+\n",
      "|[50926.776633862784,4.507953423649012,4.01,6.154788047780713,33663.6692440211]   |471824.010478477  |\n",
      "|[62173.580099008206,5.098958554487097,3.14,5.662268147045037,3883.448164008629]  |294313.0188266118 |\n",
      "|[46367.20585888387,5.290720475879383,4.5,5.181613803996049,26015.296446574837]   |266931.6886085239 |\n",
      "|[49851.134783967645,4.6849956831963695,3.04,5.259695411851781,32511.846268200865]|350030.3291141116 |\n",
      "|[51144.8509024324,4.1916217441472075,3.03,6.7018691307537654,18277.601364263126] |255785.67556118593|\n",
      "|[41240.05727656731,5.81593445999404,4.15,5.2116956265858665,40888.07856106315]   |473587.12005436234|\n",
      "|[54994.91828997551,5.186801231971905,2.2,6.063933310548773,15444.482612961785]   |379647.6377688055 |\n",
      "|[52511.654346246796,4.5798849390627705,3.3,6.561220473951403,24612.987243149037] |428528.74203875987|\n",
      "|[51634.79804563285,5.41791399103995,3.16,4.891429666076752,18998.85515745533]    |257144.20374405896|\n",
      "|[61194.47959106213,3.7119920072436483,4.07,7.091023790764967,28667.140510181758] |599800.4452090645 |\n",
      "+---------------------------------------------------------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the created spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
