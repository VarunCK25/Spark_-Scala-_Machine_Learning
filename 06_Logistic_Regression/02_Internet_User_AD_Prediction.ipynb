{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To predict whether or not a particular internet user will click on an ad based off the features of that user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement.\n",
    "We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n",
    "\n",
    "This data set contains the following features:\n",
    "\n",
    " - **`Daily Time Spent on Site`**: consumer time on site in minutes\n",
    " - **`Age`**: cutomer age in years\n",
    " - **`Area Income`**: Avg. Income of geographical area of consumer\n",
    " - **`Daily Internet Usage`**: Avg. minutes a day consumer is on the internet\n",
    " - **`Ad Topic Line`**: Headline of the advertisement\n",
    " - **`City`**: City of consumer\n",
    " - **`Male`**: Whether or not consumer was male\n",
    " - **`Country`**: Country of consumer\n",
    " - **`Timestamp`**: Time at which consumer clicked on Ad or closed window\n",
    " - **`Clicked on Ad`**: 0 or 1 indicated clicking on Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577539551875)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 18:56:04 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@187ea8d3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"Advertisement\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements to setup ML for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, OneHotEncoder}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{StringIndexer,VectorAssembler,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read in the carprices csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 8 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"advertising.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.sql.Row] = Array([68.95,35,61833.9,256.09,Cloned 5thgeneration orchestration,Wrightburgh,0,Tunisia,2016-03-27 00:53:11.0,0])\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Daily Time Spent on Site: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Area Income: double (nullable = true)\n",
      " |-- Daily Internet Usage: double (nullable = true)\n",
      " |-- Ad Topic Line: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Clicked on Ad: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|          City|Male|   Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "|                   68.95| 35|    61833.9|              256.09|Cloned 5thgenerat...|   Wrightburgh|   0|   Tunisia|2016-03-27 00:53:11|            0|\n",
      "|                   80.23| 31|   68441.85|              193.77|Monitored nationa...|     West Jodi|   1|     Nauru|2016-04-04 01:39:02|            0|\n",
      "|                   69.47| 26|   59785.94|               236.5|Organic bottom-li...|      Davidton|   0|San Marino|2016-03-13 20:35:42|            0|\n",
      "|                   74.15| 29|   54806.18|              245.89|Triple-buffered r...|West Terrifurt|   1|     Italy|2016-01-10 02:31:19|            0|\n",
      "|                   68.37| 35|   73889.99|              225.58|Robust logistical...|  South Manuel|   0|   Iceland|2016-06-03 03:36:18|            0|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting hour of the timestamp column to check whether it contains any valuable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|hour(Timestamp)|count|\n",
      "+---------------+-----+\n",
      "|              7|   54|\n",
      "|             20|   50|\n",
      "|              9|   49|\n",
      "|             21|   48|\n",
      "|              0|   45|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(hour(data(\"Timestamp\"))).count().orderBy($\"Count\".desc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the \"Timestamp\" column to numerical type by getting hour from the timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered_data: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 9 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered_data = data.withColumn(\"Hour\",hour(data(\"Timestamp\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 1000\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 1000\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out whether the string columns \"City\", \"Country\" and \"Ad Topic Line\" are useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        City|count|\n",
      "+------------+-----+\n",
      "|   Lisamouth|    3|\n",
      "|Williamsport|    3|\n",
      "|  Lake David|    2|\n",
      "|  South Lisa|    2|\n",
      "|North Daniel|    2|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.groupBy(\"City\").count().orderBy($\"Count\".desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       Country|count|\n",
      "+--------------+-----+\n",
      "|        France|    9|\n",
      "|Czech Republic|    9|\n",
      "|          Peru|    8|\n",
      "|        Turkey|    8|\n",
      "|        Greece|    8|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.groupBy(\"Country\").count().orderBy($\"Count\".desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       Ad Topic Line|count|\n",
      "+--------------------+-----+\n",
      "|Phased zero admin...|    1|\n",
      "|Customizable modu...|    1|\n",
      "|Reactive national...|    1|\n",
      "|Vision-oriented h...|    1|\n",
      "|Ergonomic client-...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.groupBy(\"Ad Topic Line\").count().orderBy($\"Count\".desc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the \"City\", \"Country\" and \"Ad Topic Line\" columns contains large number of categories, these are not much useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = filtered_data.drop(\"City\",\"Country\",\"Ad Topic Line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling all the features to a single vector column \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Daily Time Spent on Site: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Area Income: double (nullable = true)\n",
      " |-- Daily Internet Usage: double (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Clicked on Ad: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e6a36fdcc8e8\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Daily Time Spent on Site\",\"Age\",\"Area Income\",\"Daily Internet Usage\"\n",
    "                                                         ,\"Male\",\"Hour\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the resultant data into training data and testing data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<b>Training data is to train the model</b>\n",
    "<b>Testing data is to test the builted model</b>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Daily Time Spent on Site: double, Age: int ... 6 more fields]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Daily Time Spent on Site: double, Age: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data,test_data) = df.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|summary|Daily Time Spent on Site|              Age|       Area Income|Daily Internet Usage|               Male|     Clicked on Ad|             Hour|\n",
      "+-------+------------------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|  count|                    1000|             1000|              1000|                1000|               1000|              1000|             1000|\n",
      "|   mean|       65.00020000000012|           36.009| 55000.00008000003|  180.00010000000003|              0.481|               0.5|            11.66|\n",
      "| stddev|      15.853614567500212|8.785562310125924|13414.634022282358|    43.9023393019801|0.49988887654046543|0.5002501876563867|6.960952151455644|\n",
      "|    min|                    32.6|               19|           13996.5|              104.78|                  0|                 0|                0|\n",
      "|    max|                   91.43|               61|           79484.8|              269.96|                  1|                 1|               23|\n",
      "+-------+------------------------+-----------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|summary|Daily Time Spent on Site|               Age|       Area Income|Daily Internet Usage|              Male|     Clicked on Ad|              Hour|\n",
      "+-------+------------------------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|  count|                     721|               721|               721|                 721|               721|               721|               721|\n",
      "|   mean|        65.2325104022192|36.059639389736475|55102.108585298214|  180.93984743411903|0.4882108183079057|0.4895977808599168|11.515950069348127|\n",
      "| stddev|      15.841922948837537| 8.917557237943695|13475.913660724953|  43.448656245734746|0.5002080011183364|0.5002388087432303| 7.056130763602599|\n",
      "|    min|                    32.6|                19|           13996.5|               105.0|                 0|                 0|                 0|\n",
      "|    max|                   91.43|                60|          79332.33|              267.01|                 1|                 1|                23|\n",
      "+-------+------------------------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+------------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|summary|Daily Time Spent on Site|               Age|       Area Income|Daily Internet Usage|               Male|     Clicked on Ad|             Hour|\n",
      "+-------+------------------------+------------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|  count|                     279|               279|               279|                 279|                279|               279|              279|\n",
      "|   mean|       64.39985663082437|35.878136200716845|54736.128279569886|  177.57157706093193|0.46236559139784944|0.5268817204301075|12.03225806451613|\n",
      "| stddev|       15.89651735469592| 8.449140399601253| 13275.35319978117|   45.04194916468742|0.49947756414594924|0.5001740240205834|6.706785374727563|\n",
      "|    min|                    32.6|                19|          17709.98|              104.78|                  0|                 0|                0|\n",
      "|    max|                   91.15|                61|           79484.8|              269.96|                  1|                 1|               23|\n",
      "+-------+------------------------+------------------+------------------+--------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lor: org.apache.spark.ml.classification.LogisticRegression = logreg_f3ae57488577\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lor = new LogisticRegression().setLabelCol(\"Clicked on Ad\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_56e02631b389\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(assembler,lor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the pipeline to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 00:53:37 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-29 00:53:37 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_56e02631b389\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = pipelineModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Daily Time Spent on Site: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Area Income: double (nullable = true)\n",
      " |-- Daily Internet Usage: double (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Clicked on Ad: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [Clicked on Ad: int, rawPrediction: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = results.select(\"Clicked on Ad\",\"rawPrediction\",\"prediction\",\"probability\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+--------------------+--------------------+\n",
      "|Clicked on Ad|       rawPrediction|prediction|         probability|            features|\n",
      "+-------------+--------------------+----------+--------------------+--------------------+\n",
      "|            1|[-9.1403211454047...|       1.0|[1.07241378264061...|[32.6,38.0,40159....|\n",
      "|            1|[-10.588619855570...|       1.0|[2.52005419826471...|[32.84,40.0,41232...|\n",
      "|            1|[-10.003348132975...|       1.0|[4.52461316382355...|[32.99,45.0,49282...|\n",
      "|            1|[-9.3818791610223...|       1.0|[8.42296640411551...|[34.3,41.0,53167....|\n",
      "|            1|[-11.620008633307...|       1.0|[8.98442875143056...|[34.96,42.0,36913...|\n",
      "+-------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Converting the data to rdd and evaluating using MulticlassMetrics to print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_result: org.apache.spark.sql.DataFrame = [Clicked on Ad: double, rawPrediction: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clean_result = output.withColumn(\"Clicked on Ad\",output(\"Clicked on Ad\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|Clicked on Ad|prediction|\n",
      "+-------------+----------+\n",
      "|          1.0|       1.0|\n",
      "|          1.0|       1.0|\n",
      "|          1.0|       1.0|\n",
      "|          1.0|       1.0|\n",
      "|          1.0|       1.0|\n",
      "+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_result.select(\"Clicked on Ad\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[171] at rdd at <console>:51\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabel = clean_result.select(\"Clicked on Ad\",\"prediction\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f1e727\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new MulticlassMetrics(predictionAndLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130.0  7.0    \n",
      "2.0    140.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "println(metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "println(metrics.recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "println(metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Evaluating using BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bin_eval: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_46a0709642e4\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bin_eval = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Clicked on Ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOC: Double = 0.9891774891774894\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AOC =bin_eval.evaluate(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891774891774894\n"
     ]
    }
   ],
   "source": [
    "println(AOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluating using MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_261a6dc05813\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val multi_eval = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"Clicked on Ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOC_2: Double = 0.967762682621492\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AOC_2 = multi_eval.evaluate(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967762682621492\n"
     ]
    }
   ],
   "source": [
    "println(AOC_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the created spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
