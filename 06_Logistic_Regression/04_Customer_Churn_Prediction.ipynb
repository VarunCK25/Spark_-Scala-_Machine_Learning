{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now. Thus, predicting which customers will Churn(stop buying the service), which will help the agency to assign them an account manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577622954730)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 18:06:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3fc3c4cc\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"Churn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements to setup ML for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, OneHotEncoder}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{StringIndexer,VectorAssembler,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read the customer churn data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.sql.Row] = Array([Cameron Williams,42.0,11066.8,0,7.22,8.0,2013-08-30 07:00:40.0,10265 Elizabeth Mission Barkerburgh, AK 89518,Harvey LLC,1])\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
      "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 900\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 900\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.na.drop().count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out whether the string columns \"Names\", \"Location\" and \"Company\"are useful or not (to check whether they are categorical columns or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 899\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupBy(\"Names\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 900\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupBy(\"Location\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 873\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupBy(\"Company\").count().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring the categorical columns since they are not useful and checking out timestamp column \"Onboard_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       Onboard_date|\n",
      "+-------------------+\n",
      "|2013-08-30 07:00:40|\n",
      "|2013-08-13 00:38:46|\n",
      "|2016-06-29 06:20:07|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Onboard_date\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out whether `Year` is useful or not in the column `Onboard_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 11\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupBy(year($\"Onboard_date\")).count().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new column `Onboard_Year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered_data: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var filtered_data = data.withColumn(\"Onboard_Year\",year($\"Onboard_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- Onboard_Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered_data: org.apache.spark.sql.DataFrame = [Age: double, Total_Purchase: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = filtered_data.drop(\"Names\",\"Location\",\"Company\",\"Onboard_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+-----+---------+-----+------------+\n",
      "| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|Onboard_Year|\n",
      "+----+--------------+---------------+-----+---------+-----+------------+\n",
      "|42.0|       11066.8|              0| 7.22|      8.0|    1|        2013|\n",
      "|41.0|      11916.22|              0|  6.5|     11.0|    1|        2013|\n",
      "|38.0|      12884.75|              0| 6.67|     12.0|    1|        2016|\n",
      "|42.0|       8010.76|              0| 6.71|     10.0|    1|        2014|\n",
      "|37.0|       9191.58|              0| 5.56|      9.0|    1|        2016|\n",
      "+----+--------------+---------------+-----+---------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- Onboard_Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling all the features to a single vector column \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5f8d16d37bcc\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Age\",\"Total_Purchase\",\"Account_Manager\",\"Years\",\"Num_Sites\"\n",
    "                                                         ,\"Onboard_Year\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [Age: double, Total_Purchase: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_data: org.apache.spark.sql.DataFrame = [Churn: int, features: vector]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_data = output.select(\"Churn\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------+\n",
      "|Churn|features                            |\n",
      "+-----+------------------------------------+\n",
      "|1    |[42.0,11066.8,0.0,7.22,8.0,2013.0]  |\n",
      "|1    |[41.0,11916.22,0.0,6.5,11.0,2013.0] |\n",
      "|1    |[38.0,12884.75,0.0,6.67,12.0,2016.0]|\n",
      "|1    |[42.0,8010.76,0.0,6.71,10.0,2014.0] |\n",
      "|1    |[37.0,9191.58,0.0,5.56,9.0,2016.0]  |\n",
      "+-----+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the resultant data into training data and testing data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<b>Training data is to train the model</b>\n",
    "<b>Testing data is to test the builted model</b>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Churn: int, features: vector]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Churn: int, features: vector]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data,test_data) = final_data.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Churn|\n",
      "+-------+-------------------+\n",
      "|  count|                900|\n",
      "|   mean|0.16666666666666666|\n",
      "| stddev| 0.3728852122772358|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Churn|\n",
      "+-------+-------------------+\n",
      "|  count|                618|\n",
      "|   mean|0.16828478964401294|\n",
      "| stddev|0.37442204382160843|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Churn|\n",
      "+-------+-------------------+\n",
      "|  count|                282|\n",
      "|   mean|0.16312056737588654|\n",
      "| stddev|0.37013248518085234|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lor: org.apache.spark.ml.classification.LogisticRegression = logreg_d07b9689afac\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lor = new LogisticRegression().setLabelCol(\"Churn\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model and fitting the training data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 18:07:38 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-29 18:07:38 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "churnModel: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_d07b9689afac\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val churnModel = lor.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Churn: int, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = churnModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|Churn|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|[22.0,11254.38,1....|[5.27307477247574...|[0.99489834095603...|       0.0|\n",
      "|    0|[25.0,9672.03,0.0...|[4.81891927445291...|[0.99198918160809...|       0.0|\n",
      "|    0|[26.0,8939.61,0.0...|[6.36659522502375...|[0.99828494777962...|       0.0|\n",
      "|    0|[29.0,10203.18,1....|[4.29595666957292...|[0.98655957383690...|       0.0|\n",
      "|    0|[29.0,13255.05,1....|[4.70098194326566...|[0.99099546789519...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Converting the data to rdd and evaluating using MulticlassMetrics to print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_result: org.apache.spark.sql.DataFrame = [Churn: double, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clean_result = results.withColumn(\"Churn\",results(\"Churn\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Churn|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_result.select(\"Churn\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[196] at rdd at <console>:43\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabel = clean_result.select(\"Churn\",\"prediction\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@2d3ff5cb\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new MulticlassMetrics(predictionAndLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224.0  20.0  \n",
      "12.0   26.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865248226950354\n"
     ]
    }
   ],
   "source": [
    "println(metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865248226950354\n"
     ]
    }
   ],
   "source": [
    "println(metrics.recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865248226950354\n"
     ]
    }
   ],
   "source": [
    "println(metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluating using BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bin_eval: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_d99aaea2d3e1\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bin_eval = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOC: Double = 0.9057663964627868\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AOC =bin_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Evaluating using MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_cc606ec67b84\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val multi_eval = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOC_2: Double = 0.8820668693009119\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AOC_2 = multi_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8820668693009119\n"
     ]
    }
   ],
   "source": [
    "println(AOC_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on brand new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 18:08:22 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:22 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:23 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:23 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:24 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:24 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:25 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:26 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:26 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:26 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:27 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:27 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:27 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:28 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:28 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:29 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:29 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:29 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:30 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:30 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:30 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:31 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:31 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:31 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:32 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:32 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:32 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:33 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:33 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:33 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:34 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:34 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:34 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:35 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:35 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:35 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:36 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:36 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:36 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:37 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:37 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:37 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:37 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:38 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:38 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:39 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:39 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:39 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:40 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:40 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:41 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:41 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:42 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:42 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:43 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:44 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:45 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:45 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:45 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:46 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:46 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:47 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:47 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:47 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:48 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:48 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:48 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:49 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 18:08:49 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:50 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:50 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:50 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:51 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:51 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:51 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:52 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:52 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:53 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:53 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:54 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:54 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:54 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:55 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:55 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:55 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:56 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:56 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:56 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:57 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:57 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:57 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:58 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:58 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:58 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:58 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:59 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:59 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:08:59 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2019-12-29 18:09:00 ERROR LBFGS:27 - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "final_lr_Model: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_d07b9689afac\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_lr_Model = lor.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_customers: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_customers  = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"new_customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_customers_1: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var new_customers_1 = new_customers.withColumn(\"Onboard_Year\",year($\"Onboard_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_new_customers: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_new_customers  = assembler.transform(new_customers_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[org.apache.spark.sql.Row] = Array([Andrew Mccall,37.0,9935.53,1,7.71,8.0,2011-08-29 18:37:54.0,38612 Johnny Stravenue Nataliebury, WI 15717-8316,King Ltd,2011,[37.0,9935.53,1.0,7.71,8.0,2011.0]])\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_customers.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------+--------------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|Onboard_Year|            features|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------+--------------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|        2011|[37.0,9935.53,1.0...|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|        2013|[23.0,7526.94,1.0...|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|        2006|[65.0,100.0,1.0,1...|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_new_customers.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_results: org.apache.spark.sql.DataFrame = [Names: string, Age: double ... 12 more fields]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_results = final_lr_Model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       0.0|\n",
      "|Barron-Robertson|       0.0|\n",
      "|   Sexton-Golden|       0.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       0.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.select(\"Company\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So based on above predictions no need to assign Account Managers to above Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
