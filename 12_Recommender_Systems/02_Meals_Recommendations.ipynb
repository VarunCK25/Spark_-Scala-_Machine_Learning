{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meals Recommendations based on Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recommend new meals to customers based on the other customers likings for a Food Delivery Company.\n",
    "\n",
    "Final result should be in the form of a function that can take in a Spark DataFrame of a single customer's ratings for various meals and output their top 3 suggested meals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577814168238)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 23:12:44 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-12-31 23:13:03 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@20766ee\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"meals\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read the movie lens data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [movieId: int, rating: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.options(Map((\"header\",\"true\"),(\"inferSchema\",\"true\"))).csv(\"Meal_Info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------+--------------------+\n",
      "|movieId|rating|userId|mealskew|           meal_name|\n",
      "+-------+------+------+--------+--------------------+\n",
      "|      2|   3.0|     0|     2.0|       Chicken Curry|\n",
      "|      3|   1.0|     0|     3.0|Spicy Chicken Nug...|\n",
      "|      5|   2.0|     0|     5.0|           Hamburger|\n",
      "|      9|   4.0|     0|     9.0|       Taco Surprise|\n",
      "|     11|   1.0|     0|    11.0|            Meatloaf|\n",
      "+-------+------+------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|           movieId|            rating|            userId|          mealskew|          meal_name|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|              1501|              1501|              1501|               486|               1501|\n",
      "|   mean| 49.40572951365756|1.7741505662891406|14.383744170552964|15.502057613168724|               null|\n",
      "| stddev|28.937034065088994| 1.187276166124803| 8.591040424293272| 9.250633630277568|               null|\n",
      "|    min|                 0|               1.0|                 0|               0.0|           BBQ Ribs|\n",
      "|    max|                99|               5.0|                29|              31.0|Vietnamese Sandwich|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 1501\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- mealskew: double (nullable = true)\n",
      " |-- meal_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 486\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered_data: org.apache.spark.sql.DataFrame = [movieId: int, rating: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered_data = data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------+--------------------+\n",
      "|movieId|rating|userId|mealskew|           meal_name|\n",
      "+-------+------+------+--------+--------------------+\n",
      "|      2|   3.0|     0|     2.0|       Chicken Curry|\n",
      "|      3|   1.0|     0|     3.0|Spicy Chicken Nug...|\n",
      "|      5|   2.0|     0|     5.0|           Hamburger|\n",
      "|      9|   4.0|     0|     9.0|       Taco Surprise|\n",
      "|     11|   1.0|     0|    11.0|            Meatloaf|\n",
      "+-------+------+------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the dataset is smaller, we can use 80% and 20% for train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [movieId: int, rating: double ... 3 more fields]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [movieId: int, rating: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_data,test_data) = filtered_data.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|           movieId|            rating|           userId|          mealskew|          meal_name|\n",
      "+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|               486|               486|              486|               486|                486|\n",
      "|   mean|15.502057613168724|1.7366255144032923|14.46707818930041|15.502057613168724|               null|\n",
      "| stddev| 9.250633630277568|1.1808507031723887| 8.56063554474528| 9.250633630277568|               null|\n",
      "|    min|                 0|               1.0|                0|               0.0|           BBQ Ribs|\n",
      "|    max|                31|               5.0|               29|              31.0|Vietnamese Sandwich|\n",
      "+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|           movieId|            rating|            userId|          mealskew|          meal_name|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|               399|               399|               399|               399|                399|\n",
      "|   mean|15.401002506265664| 1.744360902255639|14.431077694235588|15.401002506265664|               null|\n",
      "| stddev| 9.254068723387551|1.1861055017923565| 8.363274381687813| 9.254068723387551|               null|\n",
      "|    min|                 0|               1.0|                 0|               0.0|           BBQ Ribs|\n",
      "|    max|                31|               5.0|                29|              31.0|Vietnamese Sandwich|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-----------------+-------------------+\n",
      "|summary|          movieId|            rating|            userId|         mealskew|          meal_name|\n",
      "+-------+-----------------+------------------+------------------+-----------------+-------------------+\n",
      "|  count|               87|                87|                87|               87|                 87|\n",
      "|   mean|15.96551724137931|1.7011494252873562|14.632183908045977|15.96551724137931|               null|\n",
      "| stddev|9.274180557874002|1.1625447481133553| 9.463657465110797|9.274180557874002|               null|\n",
      "|    min|                0|               1.0|                 0|              0.0|           BBQ Ribs|\n",
      "|    max|               31|               5.0|                29|             31.0|Vietnamese Sandwich|\n",
      "+-------+-----------------+------------------+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.recommendation.ALS\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the recommendation model using ALS on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_9fa1c17eed52\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val als = new ALS().setUserCol(\"userId\").setItemCol(\"mealskew\").setRatingCol(\"rating\").setMaxIter(5).setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 23:20:40 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-31 23:20:40 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-12-31 23:20:41 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-12-31 23:20:41 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.recommendation.ALSModel = als_9fa1c17eed52\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see how the model performed !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [movieId: int, rating: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Getting the predictions on test data\n",
    "val predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------+------------+----------+\n",
      "|movieId|rating|userId|mealskew|   meal_name|prediction|\n",
      "+-------+------+------+--------+------------+----------+\n",
      "|     31|   1.0|    27|    31.0|Chicken Wrap| 1.5365268|\n",
      "|     31|   4.0|    12|    31.0|Chicken Wrap|-1.5856699|\n",
      "|     31|   1.0|     5|    31.0|Chicken Wrap| 3.2605164|\n",
      "|     31|   3.0|     8|    31.0|Chicken Wrap| 1.8514191|\n",
      "|     31|   1.0|    24|    31.0|Chicken Wrap| 2.6180766|\n",
      "+-------+------+------+--------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model by computing the RMSE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_e9472da42d92\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Setting evaluator to evaluate Root Mean Squared Error\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"rating\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse: Double = 1.9823851621928843\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.98\n"
     ]
    }
   ],
   "source": [
    "println(f\"Root Mean Squared Error: ${rmse}%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The RMSE 1.98 described our error in terms of the stars rating column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for a particular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "single_user: org.apache.spark.sql.DataFrame = [userId: int, mealskew: double]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var single_user = test_data.select(\"userId\",\"mealskew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|    28|    4|\n",
      "|    27|    4|\n",
      "|    26|    3|\n",
      "|    12|    3|\n",
      "|    22|    2|\n",
      "|     1|    6|\n",
      "|    13|    2|\n",
      "|    16|    1|\n",
      "|     3|    2|\n",
      "|    20|    2|\n",
      "|     5|    4|\n",
      "|    19|    4|\n",
      "|    15|    1|\n",
      "|     9|    3|\n",
      "|    17|    1|\n",
      "|     4|    2|\n",
      "|     8|    2|\n",
      "|     7|    4|\n",
      "|    10|    6|\n",
      "|    25|    3|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "single_user: org.apache.spark.sql.DataFrame = [userId: int, mealskew: double]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_user = single_user.filter(\"userId == 26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recommendations: org.apache.spark.sql.DataFrame = [userId: int, mealskew: double ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recommendations = model.transform(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|userId|mealskew|prediction|\n",
      "+------+--------+----------+\n",
      "|    26|     1.0| 1.8736483|\n",
      "|    26|    27.0| 1.8605978|\n",
      "|    26|    23.0|0.77597076|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.orderBy(recommendations(\"prediction\").desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
