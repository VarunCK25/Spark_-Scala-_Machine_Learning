{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam SMS Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1577807752107)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 21:26:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@743de7d6\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"spam_sms\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Driver and Executor memory to 80GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@6bd27f57\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.SparkConf = org.apache.spark.SparkConf@6bd27f57\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.driver.memory\", \"8g\")\n",
    "//conf.set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, String)] = Array((spark.repl.class.outputDir,C:\\Users\\Varun\\AppData\\Local\\Temp\\tmp_16fjgpz), (spark.driver.memory,8g), (spark.master,local[*]), (spark.submit.deployMode,client), (spark.ui.showConsoleProgress,true), (spark.app.name,pyspark-shell))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.getAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark to read spam SMS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var data = spark.read.options(Map((\"header\",\"false\"),(\"inferSchema\",\"true\"),(\"delimiter\",\"\\t\"))).csv(\"SMS_Spam_Collection\\\\SMSSpamCollection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 5574\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving the header names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [class: string, text: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.withColumnRenamed(\"_c0\",\"class\").withColumnRenamed(\"_c1\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking ham and spam SMS Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  ham| 4827|\n",
      "| spam|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new length feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.length\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [class: string, text: string ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.withColumn(\"length\",length($\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "+-----+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping spam and ham sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"class\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above observation, we can conclude that if a sms is a spam, then it will be having more length as compared to ham sms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer}\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_885048ce3990\r\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_ab36cab887c9\r\n",
       "count_vec: org.apache.spark.ml.feature.CountVectorizer = cntVec_6f5a645b33a3\r\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_840de68286f9\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"token_text\")\n",
    "val remover = new StopWordsRemover().setInputCol(\"token_text\").setOutputCol(\"stop_tokens\")\n",
    "val count_vec = new CountVectorizer().setInputCol(\"stop_tokens\").setOutputCol(\"c_vec\")\n",
    "val idf = new IDF().setInputCol(\"c_vec\").setOutputCol(\"tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the categorical data type class (ham-spam) to numerical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham_spam_to_num: org.apache.spark.ml.feature.StringIndexer = strIdx_7fdfc0712563\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ham_spam_to_num = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4b0b3b5f3361\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"tf_idf\",\"length\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{NaiveBayes, LogisticRegression, DecisionTreeClassifier}\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{NaiveBayes,LogisticRegression,DecisionTreeClassifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_5985ae8a8e88\r\n",
       "lor: org.apache.spark.ml.classification.LogisticRegression = logreg_a8c8c4cf222b\r\n",
       "dtc: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_f4c9373a4ef4\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb = new NaiveBayes().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "val lor = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "val dtc = new DecisionTreeClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_nb: org.apache.spark.ml.Pipeline = pipeline_1bc6a2b0f5bf\r\n",
       "pipeline_lor: org.apache.spark.ml.Pipeline = pipeline_dd9bc9a26553\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_nb = new Pipeline().setStages(Array(tokenizer, remover, count_vec, idf, ham_spam_to_num, assembler, nb))\n",
    "val pipeline_lor = new Pipeline().setStages(Array(tokenizer, remover, count_vec, idf, ham_spam_to_num, assembler, lor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 21:26:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-12-31 21:26:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nb_model: org.apache.spark.ml.PipelineModel = pipeline_1bc6a2b0f5bf\r\n",
       "lor_model: org.apache.spark.ml.PipelineModel = pipeline_dd9bc9a26553\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_model = pipeline_nb.fit(data)\n",
    "val lor_model = pipeline_lor.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_data_nb: org.apache.spark.sql.DataFrame = [class: string, text: string ... 10 more fields]\r\n",
       "clean_data_lor: org.apache.spark.sql.DataFrame = [class: string, text: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var clean_data_nb = nb_model.transform(data)\n",
    "var clean_data_lor = lor_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|class|                text|length|          token_text|         stop_tokens|               c_vec|              tf_idf|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  ham|Go until jurong p...|   111|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|  0.0|(13424,[7,11,31,6...|[-1000.1575901830...|[1.0,9.5553840178...|       0.0|\n",
      "|  ham|Ok lar... Joking ...|    29|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|  0.0|(13424,[0,24,297,...|[-299.78954560145...|[1.0,2.5831095772...|       0.0|\n",
      "| spam|Free entry in 2 a...|   155|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|  1.0|(13424,[2,13,19,3...|[-1674.3517542037...|[7.42583680456230...|       1.0|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data_nb.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|class|                text|length|          token_text|         stop_tokens|               c_vec|              tf_idf|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  ham|Go until jurong p...|   111|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|  0.0|(13424,[7,11,31,6...|[28.2282693346141...|[0.99999999999944...|       0.0|\n",
      "|  ham|Ok lar... Joking ...|    29|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|  0.0|(13424,[0,24,297,...|[26.1076534066338...|[0.99999999999541...|       0.0|\n",
      "| spam|Free entry in 2 a...|   155|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|  1.0|(13424,[2,13,19,3...|[-19.549034263138...|[3.23565561529989...|       1.0|\n",
      "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data_lor.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_data_nb: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n",
       "clean_data_lor: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_nb = clean_data_nb.select(\"label\",\"features\")\n",
    "clean_data_lor = clean_data_lor.select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data_nb.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data_lor.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_nb: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "test_nb: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "train_lor: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "test_lor: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_nb,test_nb) = clean_data_nb.randomSplit(Array(0.7,0.3), seed=12345)\n",
    "val Array(train_lor,test_lor) = clean_data_lor.randomSplit(Array(0.7,0.3), seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam_predictor_nb: org.apache.spark.ml.classification.NaiveBayesModel = NaiveBayesModel (uid=nb_5985ae8a8e88) with 2 classes\r\n",
       "spam_predictor_lor: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_a8c8c4cf222b\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spam_predictor_nb = nb.fit(train_nb)\n",
    "val spam_predictor_lor = lor.fit(train_lor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_results_nb: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\r\n",
       "test_results_lor: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_results_nb = spam_predictor_nb.transform(test_nb)\n",
    "val test_results_lor = spam_predictor_lor.transform(test_lor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,4,50,...|[-819.92914430065...|[1.0,9.2949203991...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,15,...|[-997.90523073055...|[1.0,1.6286640272...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-874.03499580029...|[1.0,1.8162872740...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_nb.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,4,50,...|[39.9998544815758...|[1.0,4.2489725140...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,15,...|[48.0074481259798...|[1.0,1.4145887133...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[29.8269701451849...|[0.99999999999988...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_lor.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_9ae58527c216\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acc_eval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Model at predictiong spam was: 0.92\n",
      "********************************************************************************\n",
      "Accuracy of Random Forest Classifier Model at predictiong spam was: 0.96\n",
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "acc_nb: Double = 0.9246423704291925\r\n",
       "acc_lor: Double = 0.9636361033474314\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acc_nb = acc_eval.evaluate(test_results_nb)\n",
    "println(f\"Accuracy of Naive Bayes Model at predictiong spam was: ${acc_nb}%1.2f\")\n",
    "println(\"*\"*80)\n",
    "\n",
    "val acc_lor = acc_eval.evaluate(test_results_lor)\n",
    "println(f\"Accuracy of Random Forest Classifier Model at predictiong spam was: ${acc_lor}%1.2f\")\n",
    "println(\"*\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[295] at rdd at <console>:51\r\n",
       "lor_predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[299] at rdd at <console>:52\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_predictionAndLabel = test_results_nb.select(\"prediction\",\"label\").as[(Double,Double)].rdd\n",
    "val lor_predictionAndLabel = test_results_lor.select(\"prediction\",\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@11c75c67\r\n",
       "lor_metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@673345ba\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_metrics = new MulticlassMetrics(nb_predictionAndLabel)\n",
    "val lor_metrics = new MulticlassMetrics(lor_predictionAndLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model - Confusion Matrix:\n",
      "1346.0  131.0  \n",
      "9.0     208.0  \n"
     ]
    }
   ],
   "source": [
    "println(\"Naive Bayes Model - Confusion Matrix:\")\n",
    "println(nb_metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Confusion Matrix:\n",
      "1475.0  2.0    \n",
      "56.0    161.0  \n"
     ]
    }
   ],
   "source": [
    "println(\"Logistic Regression Model - Confusion Matrix:\")\n",
    "println(lor_metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model - Accuracy:\n",
      "0.9173553719008265\n"
     ]
    }
   ],
   "source": [
    "println(\"Naive Bayes Model - Accuracy:\")\n",
    "println(nb_metrics.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Accuracy:\n",
      "0.9657615112160567\n"
     ]
    }
   ],
   "source": [
    "println(\"Logistic Regression Model - Accuracy:\")\n",
    "println(lor_metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model - Precision:\n",
      "0.9173553719008265\n"
     ]
    }
   ],
   "source": [
    "println(\"Naive Bayes Model - Precision:\")\n",
    "println(nb_metrics.precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Precision:\n",
      "0.9657615112160567\n"
     ]
    }
   ],
   "source": [
    "println(\"Logistic Regression Model - Precision:\")\n",
    "println(lor_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model - Recall:\n",
      "0.9173553719008265\n"
     ]
    }
   ],
   "source": [
    "println(\"Naive Bayes Model - Recall:\")\n",
    "println(nb_metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Recall:\n",
      "0.9657615112160567\n"
     ]
    }
   ],
   "source": [
    "println(\"Logistic Regression Model - Recall:\")\n",
    "println(lor_metrics.recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLosing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
